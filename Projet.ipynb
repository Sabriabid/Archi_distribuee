{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15dd51e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.types import IntegerType, ArrayType, BooleanType, StringType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer,NGram,HashingTF,IDF\n",
    "from pyspark.sql.functions import concat,col\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit,CrossValidator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bb261f",
   "metadata": {},
   "source": [
    "Connexion Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d9827d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"test\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a160f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43374"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import du fichier CSV:\n",
    "\n",
    "df = spark.read.option(\"delimiter\", \";\").option(\"header\", True).csv(\"Twitter.csv\")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8fa71fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43013"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfn=df.dropna()\n",
    "dfn.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de5f382d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+---------+--------------------+----+\n",
      "|Tweet ID|     entity|sentiment|       Tweet content|Note|\n",
      "+--------+-----------+---------+--------------------+----+\n",
      "|    2401|Borderlands| Positive|im getting on bor...|   1|\n",
      "|    2401|Borderlands| Positive|I am coming to th...|   1|\n",
      "|    2401|Borderlands| Positive|im getting on bor...|   1|\n",
      "|    2401|Borderlands| Positive|im coming on bord...|   1|\n",
      "|    2401|Borderlands| Positive|im getting on bor...|   1|\n",
      "|    2401|Borderlands| Positive|im getting into b...|   1|\n",
      "|    2402|Borderlands| Positive|So I spent a few ...|   1|\n",
      "|    2402|Borderlands| Positive|So I spent a coup...|   1|\n",
      "|    2402|Borderlands| Positive|So I spent a few ...|   1|\n",
      "|    2402|Borderlands| Positive|So I spent a few ...|   1|\n",
      "|    2402|Borderlands| Positive|2010 So I spent a...|   1|\n",
      "|    2402|Borderlands| Positive|                 was|   1|\n",
      "|    2404|Borderlands| Positive|that was the firs...|   1|\n",
      "|    2404|Borderlands| Positive|this was the firs...|   1|\n",
      "|    2404|Borderlands| Positive|that was the firs...|   1|\n",
      "|    2404|Borderlands| Positive|that was the firs...|   1|\n",
      "|    2404|Borderlands| Positive|that I was the fi...|   1|\n",
      "|    2404|Borderlands| Positive|that was the firs...|   1|\n",
      "|    2405|Borderlands| Negative|the biggest dissa...|   0|\n",
      "|    2405|Borderlands| Negative|The biggest disap...|   0|\n",
      "+--------+-----------+---------+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = dfn.withColumn(\"Note\", when(dfn.sentiment ==\"Positive\" ,1)\n",
    "                                                    .when(dfn.sentiment==\"Negative\" ,0))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edcde95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+---------+--------------------+----+--------------------+\n",
      "|Tweet ID|     entity|sentiment|       Tweet content|Note|               words|\n",
      "+--------+-----------+---------+--------------------+----+--------------------+\n",
      "|    2401|Borderlands| Positive|im getting on bor...|   1|[im, getting, on,...|\n",
      "|    2401|Borderlands| Positive|I am coming to th...|   1|[i, am, coming, t...|\n",
      "|    2401|Borderlands| Positive|im getting on bor...|   1|[im, getting, on,...|\n",
      "|    2401|Borderlands| Positive|im coming on bord...|   1|[im, coming, on, ...|\n",
      "|    2401|Borderlands| Positive|im getting on bor...|   1|[im, getting, on,...|\n",
      "|    2401|Borderlands| Positive|im getting into b...|   1|[im, getting, int...|\n",
      "|    2402|Borderlands| Positive|So I spent a few ...|   1|[so, i, spent, a,...|\n",
      "|    2402|Borderlands| Positive|So I spent a coup...|   1|[so, i, spent, a,...|\n",
      "|    2402|Borderlands| Positive|So I spent a few ...|   1|[so, i, spent, a,...|\n",
      "|    2402|Borderlands| Positive|So I spent a few ...|   1|[so, i, spent, a,...|\n",
      "|    2402|Borderlands| Positive|2010 So I spent a...|   1|[2010, so, i, spe...|\n",
      "|    2402|Borderlands| Positive|                 was|   1|               [was]|\n",
      "|    2404|Borderlands| Positive|that was the firs...|   1|[that, was, the, ...|\n",
      "|    2404|Borderlands| Positive|this was the firs...|   1|[this, was, the, ...|\n",
      "|    2404|Borderlands| Positive|that was the firs...|   1|[that, was, the, ...|\n",
      "|    2404|Borderlands| Positive|that was the firs...|   1|[that, was, the, ...|\n",
      "|    2404|Borderlands| Positive|that I was the fi...|   1|[that, i, was, th...|\n",
      "|    2404|Borderlands| Positive|that was the firs...|   1|[that, was, the, ...|\n",
      "|    2405|Borderlands| Negative|the biggest dissa...|   0|[the, biggest, di...|\n",
      "|    2405|Borderlands| Negative|The biggest disap...|   0|[the, biggest, di...|\n",
      "+--------+-----------+---------+--------------------+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Tokenzed\n",
    "tokenizer = Tokenizer(inputCol=\"Tweet content\", outputCol=\"words\")\n",
    "tokenized = tokenizer.transform(df1)\n",
    "tokenized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cb207a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+---------+--------------------+----+--------------------+--------------------+\n",
      "|Tweet ID|     entity|sentiment|       Tweet content|Note|               words|            Resultat|\n",
      "+--------+-----------+---------+--------------------+----+--------------------+--------------------+\n",
      "|    2401|Borderlands| Positive|im getting on bor...|   1|[im, getting, on,...|[im, getting, bor...|\n",
      "|    2401|Borderlands| Positive|I am coming to th...|   1|[i, am, coming, t...|[coming, borders,...|\n",
      "|    2401|Borderlands| Positive|im getting on bor...|   1|[im, getting, on,...|[im, getting, bor...|\n",
      "|    2401|Borderlands| Positive|im coming on bord...|   1|[im, coming, on, ...|[im, coming, bord...|\n",
      "|    2401|Borderlands| Positive|im getting on bor...|   1|[im, getting, on,...|[im, getting, bor...|\n",
      "|    2401|Borderlands| Positive|im getting into b...|   1|[im, getting, int...|[im, getting, bor...|\n",
      "|    2402|Borderlands| Positive|So I spent a few ...|   1|[so, i, spent, a,...|[spent, hours, ma...|\n",
      "|    2402|Borderlands| Positive|So I spent a coup...|   1|[so, i, spent, a,...|[spent, couple, h...|\n",
      "|    2402|Borderlands| Positive|So I spent a few ...|   1|[so, i, spent, a,...|[spent, hours, so...|\n",
      "|    2402|Borderlands| Positive|So I spent a few ...|   1|[so, i, spent, a,...|[spent, hours, ma...|\n",
      "|    2402|Borderlands| Positive|2010 So I spent a...|   1|[2010, so, i, spe...|[2010, spent, hou...|\n",
      "|    2402|Borderlands| Positive|                 was|   1|               [was]|                  []|\n",
      "|    2404|Borderlands| Positive|that was the firs...|   1|[that, was, the, ...|[first, borderlan...|\n",
      "|    2404|Borderlands| Positive|this was the firs...|   1|[this, was, the, ...|[first, borderlan...|\n",
      "|    2404|Borderlands| Positive|that was the firs...|   1|[that, was, the, ...|[first, borderlan...|\n",
      "|    2404|Borderlands| Positive|that was the firs...|   1|[that, was, the, ...|[first, borderlan...|\n",
      "|    2404|Borderlands| Positive|that I was the fi...|   1|[that, i, was, th...|[first, real, bor...|\n",
      "|    2404|Borderlands| Positive|that was the firs...|   1|[that, was, the, ...|[first, borderlan...|\n",
      "|    2405|Borderlands| Negative|the biggest dissa...|   0|[the, biggest, di...|[biggest, dissapp...|\n",
      "|    2405|Borderlands| Negative|The biggest disap...|   0|[the, biggest, di...|[biggest, disappo...|\n",
      "+--------+-----------+---------+--------------------+----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Stop Words\n",
    "remover = StopWordsRemover()\n",
    "remover.setInputCol(\"words\")\n",
    "remover.setOutputCol(\"Resultat\")\n",
    "df= remover.transform(tokenized)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a7a745d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               Ngram|\n",
      "+--------------------+\n",
      "|[im getting, gett...|\n",
      "|[coming borders, ...|\n",
      "|[im getting, gett...|\n",
      "|[im coming, comin...|\n",
      "|[im getting, gett...|\n",
      "|[im getting, gett...|\n",
      "|[spent hours, hou...|\n",
      "|[spent couple, co...|\n",
      "|[spent hours, hou...|\n",
      "|[spent hours, hou...|\n",
      "|[2010 spent, spen...|\n",
      "|                  []|\n",
      "|[first borderland...|\n",
      "|[first borderland...|\n",
      "|[first borderland...|\n",
      "|[first borderland...|\n",
      "|[first real, real...|\n",
      "|[first borderland...|\n",
      "|[biggest dissappo...|\n",
      "|[biggest disappoi...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Ngram\n",
    "ngram = NGram(n=2)\n",
    "ngram.setInputCol(\"Resultat\")\n",
    "ngram.setOutputCol(\"Ngram\")\n",
    "ngramDataFrame = ngram.transform(df)\n",
    "ngramDataFrame.select(\"Ngram\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31ead7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------------------------------------------------------+\n",
      "|features           |Ngram                                                          |\n",
      "+-------------------+---------------------------------------------------------------+\n",
      "|(2,[0,1],[1.0,3.0])|[im getting, getting borderlands, borderlands murder, murder ,]|\n",
      "|(2,[1],[3.0])      |[coming borders, borders kill, kill all,]                      |\n",
      "|(2,[0,1],[1.0,3.0])|[im getting, getting borderlands, borderlands kill, kill all,] |\n",
      "+-------------------+---------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Hashing\n",
    "hashingTF = HashingTF(inputCol=\"Ngram\", outputCol=\"features\")\n",
    "hashingTF.setNumFeatures(2)\n",
    "\n",
    "ls=hashingTF.transform(ngramDataFrame)\n",
    "ls.select(\"features\",\"Ngram\").show(3,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c828c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 idf|\n",
      "+--------------------+\n",
      "|(2,[0,1],[0.12567...|\n",
      "|(2,[1],[0.3713448...|\n",
      "|(2,[0,1],[0.12567...|\n",
      "|(2,[0,1],[0.37703...|\n",
      "|(2,[0,1],[0.25135...|\n",
      "|(2,[0,1],[0.25135...|\n",
      "|(2,[0,1],[1.13110...|\n",
      "|(2,[0,1],[1.50813...|\n",
      "|(2,[0,1],[0.50271...|\n",
      "|(2,[0,1],[1.00542...|\n",
      "|(2,[0,1],[1.13110...|\n",
      "|           (2,[],[])|\n",
      "|(2,[0,1],[1.25677...|\n",
      "|(2,[0,1],[1.00542...|\n",
      "|(2,[0,1],[1.25677...|\n",
      "|(2,[0,1],[1.38245...|\n",
      "|(2,[0,1],[1.50813...|\n",
      "|(2,[0,1],[0.62838...|\n",
      "|(2,[0,1],[0.75406...|\n",
      "|(2,[0,1],[0.50271...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#IDF\n",
    "idf = IDF(inputCol=\"features\", outputCol=\"idf\")\n",
    "idfModel = idf.fit(ls)\n",
    "rescaledData = idfModel.transform(ls)\n",
    "\n",
    "rescaledData.select(\"idf\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08a50c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12879"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split\n",
    "splits = rescaledData.randomSplit([0.7, 0.3], 24)\n",
    "pdf1=splits[1]\n",
    "pdf0=splits[0]\n",
    "pdf1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c99370a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LogisticRegression\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8,featuresCol='idf',\n",
    "    labelCol='Note')\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(pdf1)\n",
    "lrModel=lrModel.transform(pdf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45c80cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4482610883029995"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BinaryClassificationEvaluator\n",
    "evaluator2 = BinaryClassificationEvaluator(labelCol=\"Note\", rawPredictionCol=\"idf\", metricName='areaUnderROC')\n",
    "evaluator2.evaluate(lrModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f390fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipline\n",
    "pipeline = Pipeline(stages=[tokenizer,remover,ngram, hashingTF, idf , lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a1d2a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "94ded62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+---------+--------------------+----+\n",
      "|Tweet ID|     entity|sentiment|       Tweet content|Note|\n",
      "+--------+-----------+---------+--------------------+----+\n",
      "|    2401|Borderlands| Positive|im getting on bor...|   1|\n",
      "|    2401|Borderlands| Positive|I am coming to th...|   1|\n",
      "|    2401|Borderlands| Positive|im getting on bor...|   1|\n",
      "|    2401|Borderlands| Positive|im coming on bord...|   1|\n",
      "|    2401|Borderlands| Positive|im getting on bor...|   1|\n",
      "|    2401|Borderlands| Positive|im getting into b...|   1|\n",
      "|    2402|Borderlands| Positive|So I spent a few ...|   1|\n",
      "|    2402|Borderlands| Positive|So I spent a coup...|   1|\n",
      "|    2402|Borderlands| Positive|So I spent a few ...|   1|\n",
      "|    2402|Borderlands| Positive|So I spent a few ...|   1|\n",
      "|    2402|Borderlands| Positive|2010 So I spent a...|   1|\n",
      "|    2402|Borderlands| Positive|                 was|   1|\n",
      "|    2404|Borderlands| Positive|that was the firs...|   1|\n",
      "|    2404|Borderlands| Positive|this was the firs...|   1|\n",
      "|    2404|Borderlands| Positive|that was the firs...|   1|\n",
      "|    2404|Borderlands| Positive|that was the firs...|   1|\n",
      "|    2404|Borderlands| Positive|that I was the fi...|   1|\n",
      "|    2404|Borderlands| Positive|that was the firs...|   1|\n",
      "|    2405|Borderlands| Negative|the biggest dissa...|   0|\n",
      "|    2405|Borderlands| Negative|The biggest disap...|   0|\n",
      "+--------+-----------+---------+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(df1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
